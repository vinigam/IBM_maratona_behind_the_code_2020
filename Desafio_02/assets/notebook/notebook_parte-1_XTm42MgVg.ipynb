{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 2: PARTE 1"}, {"metadata": {}, "cell_type": "markdown", "source": "### Introdu\u00e7\u00e3o"}, {"metadata": {}, "cell_type": "markdown", "source": "Em projetos de ci\u00eancia de dados visando a constru\u00e7\u00e3o de modelos de *machine learning*, ou aprendizado estat\u00edstico, \u00e9 muito incomum que os dados iniciais estejam j\u00e1 no formato ideal para a constru\u00e7\u00e3o de modelos. S\u00e3o necess\u00e1rios v\u00e1rios passos intermedi\u00e1rios de pr\u00e9-processamento de dados, como por exemplo a codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas, normaliza\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas, tratamento de dados faltantes, etc. A biblioteca **scikit-learn** -- uma das mais populares bibliotecas de c\u00f3digo-aberto para *machine learning* no mundo -- possui diversas fun\u00e7\u00f5es j\u00e1 integradas para a realiza\u00e7\u00e3o das transforma\u00e7\u00f5es de dados mais utilizadas. Entretanto, em um fluxo comum de um modelo de aprendizado de m\u00e1quina, \u00e9 necess\u00e1ria a aplica\u00e7\u00e3o dessas transforma\u00e7\u00f5es pelo menos duas vezes: a primeira vez para \"treinar\" o modelo, e depois novamente quando novos dados forem enviados como entrada para serem classificados por este modelo. \n\nPara facilitar o trabalho com esse tipo de fluxo, o scikit-learn possui tamb\u00e9m uma ferramenta chamada **Pipeline**, que nada mais \u00e9 do que uma lista ordenada de transforma\u00e7\u00f5es que devem ser aplicadas nos dados. Para auxiliar no desenvolvimento e no gerenciamento de todo o ciclo-de-vida dessas aplica\u00e7\u00f5es, alem do uso de Pipelines, as equipes de cientistas de dados podem utilizar em conjunto o **Watson Machine Learning**, que possui dezenas de ferramentas para treinar, gerenciar, hospedar e avaliar modelos baseados em aprendizado de m\u00e1quina. Al\u00e9m disso, o Watson Machine Learning \u00e9 capaz de encapsular pipelines e modelos em uma API pronta para uso e integra\u00e7\u00e3o com outras aplica\u00e7\u00f5es.\n\nDurante o desafio 2, voc\u00ea participante ir\u00e1 aprender a construir uma **Pipeline** para um modelo de classifica\u00e7\u00e3o e hosped\u00e1-lo como uma API com o aux\u00edlio do Watson Machine Learning. Uma vez hospedado, voc\u00ea poder\u00e1 integrar o modelo criado com outras aplica\u00e7\u00f5es, como assistentes virtuais e muito mais. Neste notebook, ser\u00e1 apresentado um exemplo funcional de cria\u00e7\u00e3o de um modelo e de uma pipeline no scikit-learn (que voc\u00ea poder\u00e1 utilizar como template para a sua solu\u00e7\u00e3o!)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Trabalhando com Pipelines do scikit-learn"}, {"metadata": {}, "cell_type": "code", "source": "# Primeiro, realizamos a instala\u00e7\u00e3o do scikit-learn vers\u00e3o 0.20.0 no Kernel deste notebook:\n!pip install scikit-learn==0.20.0 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Em seguida iremos importar diversas bibliotecas que ser\u00e3o utilizadas:\n\n# Pacote para trabalhar com JSON\nimport json\n\n# Pacote para realizar requisi\u00e7\u00f5es HTTP\nimport requests\n\n# Pacote para explora\u00e7\u00e3o e an\u00e1lise de dados\nimport pandas as pd\n\n# Pacote com m\u00e9todos num\u00e9ricos e representa\u00e7\u00f5es matriciais\nimport numpy as np\n\n# Pacote para constru\u00e7\u00e3o de modelo baseado na t\u00e9cnica Gradient Boosting\nimport xgboost as xgb\n\n# Pacotes do scikit-learn para pr\u00e9-processamento de dados\n# \"SimpleImputer\" \u00e9 uma transforma\u00e7\u00e3o para preencher valores faltantes em conjuntos de dados\nfrom sklearn.impute import SimpleImputer\n\n# Pacotes do scikit-learn para treinamento de modelos e constru\u00e7\u00e3o de pipelines\n# M\u00e9todo para separa\u00e7\u00e3o de conjunto de dados em amostras de treino e teste\nfrom sklearn.model_selection import train_test_split\n# M\u00e9todo para cria\u00e7\u00e3o de modelos baseados em \u00e1rvores de decis\u00e3o\nfrom sklearn.tree import DecisionTreeClassifier\n# Classe para a cria\u00e7\u00e3o de uma pipeline de machine-learning\nfrom sklearn.pipeline import Pipeline\n\n# Pacotes do scikit-learn para avalia\u00e7\u00e3o de modelos\n# M\u00e9todos para valida\u00e7\u00e3o cruzada do modelo criado\nfrom sklearn.model_selection import KFold, cross_validate", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Importando um .csv de seu projeto no IBM Cloud Pak for Data para o Kernel deste notebook"}, {"metadata": {}, "cell_type": "markdown", "source": "Primeiro iremos importar o dataset fornecido para o desafio, que j\u00e1 est\u00e1 incluso neste projeto!\n\nVoc\u00ea pode realizar a importa\u00e7\u00e3o dos dados de um arquivo .csv diretamente para o Kernel do notebook como um DataFrame da biblioteca Pandas, muito utilizada para a manipula\u00e7\u00e3o de dados em Python.\n\nPara realizar a importa\u00e7\u00e3o, basta selecionar a pr\u00f3xima c\u00e9lula e seguir as instru\u00e7\u00f5es na imagem abaixo:\n\n![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n\nAp\u00f3s a sele\u00e7\u00e3o da op\u00e7\u00e3o **\"Insert to code\"**, a c\u00e9lula abaixo ser\u00e1 preenchida com o c\u00f3digo necess\u00e1rio para importa\u00e7\u00e3o e leitura dos dados no arquivo .csv como um DataFrame Pandas."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "\n<< INSIRA O DATASET COMO UM PANDAS DATAFRAME NESTA C\u00c9LULA! >>>\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Temos 15 colunas presentes no dataset fornecido, sendo dezessete delas vari\u00e1veis caracter\u00edsticas (dados de entrada) e um delas uma vari\u00e1vel-alvo (que queremos que o nosso modelo seja capaz de prever). \n\nAs vari\u00e1veis caracter\u00edsticas s\u00e3o:\n\n    MATRICULA       - n\u00famero de matr\u00edcula do estudante\n    NOME            - nome completo do estudante\n    REPROVACOES_DE  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Direito Empresarial``\n    REPROVACOES_EM  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Empreendedorismo``\n    REPROVACOES_MF  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Matem\u00e1tica Financeira``\n    REPROVACOES_GO  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Gest\u00e3o Operacional``\n    NOTA_DE         - m\u00e9dia simples das notas do aluno na disciplina de ``Direito Empresarial`` (0-10)\n    NOTA_EM         - m\u00e9dia simples das notas do aluno na disciplina de ``Empreendedorismo`` (0-10)\n    NOTA_MF         - m\u00e9dia simples das notas do aluno na disciplina de ``Matem\u00e1tica Financeira`` (0-10)\n    NOTA_GO         - m\u00e9dia simples das notas do aluno na disciplina de ``Gest\u00e3o Operacional`` (0-10)\n    INGLES          - vari\u00e1vel bin\u00e1ria que indica se o estudante tem conhecimento em l\u00edngua inglesa (0 -> sim ou 1 -> n\u00e3o).\n    H_AULA_PRES     - horas de estudo presencial realizadas pelo estudante\n    TAREFAS_ONLINE  - n\u00famero de tarefas online entregues pelo estudante\n    FALTAS          - n\u00famero de faltas acumuladas do estudante (todas disciplinas)\n    \nA vari\u00e1vel-alvo \u00e9:\n\n    PERFIL               - uma *string* que indica uma de cinco possibilidades: \n        \"EXCELENTE\"      - Estudante n\u00e3o necessita de mentoria\n        \"MUITO BOM\"      - Estudante n\u00e3o necessita de mentoria\n        \"HUMANAS\"        - Estudante necessita de mentoria exclusivamente em mat\u00e9rias com conte\u00fado de ci\u00eancias humanas\n        \"EXATAS\"         - Estudante necessita de mentoria apenas em disciplinas com conte\u00fado de ci\u00eancias exatas\n        \"DIFICULDADE\"    - Estudante necessita de mentoria em duas ou mais disciplinas\n        \nCom um modelo capaz de classificar um estudante em uma dessas categorias, podemos automatizar parte da mentoria estudantil atrav\u00e9s de assistentes virtuais, que ser\u00e3o capazes de recomendar pr\u00e1ticas de estudo e conte\u00fado personalizado com base nas necessidades de cada aluno."}, {"metadata": {}, "cell_type": "markdown", "source": "### Explorando os dados fornecidos\n\nPodemos continuar a explora\u00e7\u00e3o dos dados fornecidos com a fun\u00e7\u00e3o ``info()``:"}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\u00c9 notado que existem vari\u00e1veis do tipo ``float64`` (n\u00fameros \"decimais\"), vari\u00e1veis do tipo ``int64`` (n\u00fameros inteiros) e do tipo ``object`` (nesse caso s\u00e3o *strings*, ou texto). \n\nComo a maioria dos algoritmos de aprendizado estat\u00edstico supervisionado s\u00f3 aceita valores num\u00e9ricos como entrada, \u00e9 necess\u00e1rio ent\u00e3o o pr\u00e9-processamento das vari\u00e1veis do tipo \"object\" antes de usar esse dataset como entrada para o treinamento de um modelo. Tamb\u00e9m \u00e9 notado que existem valores faltantes em v\u00e1rias colunas. Esses valores faltantes tamb\u00e9m devem ser tratados antes de serem constru\u00eddos modelos com esse conjunto de dados base."}, {"metadata": {}, "cell_type": "markdown", "source": "A fun\u00e7\u00e3o ``describe()`` gera v\u00e1rias informa\u00e7\u00f5es sobre as vari\u00e1veis num\u00e9ricas que tamb\u00e9m podem ser \u00fateis:"}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Visualiza\u00e7\u00f5es\n\nPara visualizar o dataset fornecido, podemos utilizar as bibliotecas ``matplotlib`` e ``seaborn``:"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.countplot(ax=axes[0], x='REPROVACOES_DE', data=df_data_1)\nsns.countplot(ax=axes[1], x='REPROVACOES_EM', data=df_data_1)\nsns.countplot(ax=axes[2], x='REPROVACOES_MF', data=df_data_1)\nsns.countplot(ax=axes[3], x='REPROVACOES_GO', data=df_data_1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.distplot(df_data_1['NOTA_DE'], ax=axes[0])\nsns.distplot(df_data_1['NOTA_EM'], ax=axes[1])\nsns.distplot(df_data_1['NOTA_MF'], ax=axes[2])\nsns.distplot(df_data_1['NOTA_GO'].dropna(), ax=axes[3])", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.countplot(ax=axes[0], x='INGLES', data=df_data_1)\nsns.countplot(ax=axes[1], x='FALTAS', data=df_data_1)\nsns.countplot(ax=axes[2], x='H_AULA_PRES', data=df_data_1)\nsns.countplot(ax=axes[3], x='TAREFAS_ONLINE', data=df_data_1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig = plt.plot()\nsns.countplot(x='PERFIL', data=df_data_1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Realizando o pr\u00e9-processamento dos dados"}, {"metadata": {}, "cell_type": "markdown", "source": "Para o pr\u00e9-processamento dos dados ser\u00e3o apresentadas duas transforma\u00e7\u00f5es b\u00e1sicas neste notebook, demonstrando a constru\u00e7\u00e3o de uma Pipeline com um modelo funcional. Esta Pipeline funcional fornecida dever\u00e1 ser melhorada pelo participante para que o modelo final alcance a maior acur\u00e1cia poss\u00edvel, garantindo uma pontua\u00e7\u00e3o maior no desafio. Essa melhoria pode ser feita apenas no pr\u00e9-processamento dos dados, na escolha de um algoritmo para treinamento de modelo diferente, ou at\u00e9 mesmo na altera\u00e7\u00e3o do *framework* usado (entretanto s\u00f3 ser\u00e1 fornecido um exemplo pronto de integra\u00e7\u00e3o do Watson Machine Learning com o *scikit-learn*).\n\nA primeira transforma\u00e7\u00e3o (passo na nossa Pipeline) ser\u00e1 a exclus\u00e3o da coluna \"NOME\" do nosso dataset, que al\u00e9m de n\u00e3o ser uma vari\u00e1vel num\u00e9rica, tamb\u00e9m n\u00e3o \u00e9 uma vari\u00e1vel relacionada ao desempenho dos estudantes nas disciplinas. Existem fun\u00e7\u00f5es prontas no scikit-learn para a realiza\u00e7\u00e3o dessa transforma\u00e7\u00e3o, entretanto nosso exemplo ir\u00e1 demonstrar como criar uma transforma\u00e7\u00e3o personalizada do zero no scikit-learn. Se desejado, o participante poder\u00e1 utilizar esse exemplo para criar outras transforma\u00e7\u00f5es e adicion\u00e1-las \u00e0 Pipeline final :)"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Transforma\u00e7\u00e3o 1: excluindo colunas do dataset\n\nPara a cria\u00e7\u00e3o de uma transforma\u00e7\u00e3o de dados personalizada no scikit-learn, \u00e9 necess\u00e1ria basicamente a cria\u00e7\u00e3o de uma classe com os m\u00e9todos ``transform`` e ``fit``. No m\u00e9todo transform ser\u00e1 executada a l\u00f3gica da nossa transforma\u00e7\u00e3o.\n\nNa pr\u00f3xima c\u00e9lula \u00e9 apresentado o c\u00f3digo completo de uma transforma\u00e7\u00e3o ``DropColumns`` para a remo\u00e7\u00e3o de colunas de um DataFrame pandas."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\n\n# All sklearn Transforms must have the `transform` and `fit` methods\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Primeiro realizamos a c\u00f3pia do dataframe 'X' de entrada\n        data = X.copy()\n        # Retornamos um novo dataframe sem as colunas indesejadas\n        return data.drop(labels=self.columns, axis='columns')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Para aplicar essa transforma\u00e7\u00e3o em um DataFrame pandas, basta instanciar um objeto *DropColumns* e chamar o m\u00e9todo transform()."}, {"metadata": {}, "cell_type": "code", "source": "# Instanciando uma transforma\u00e7\u00e3o DropColumns\nrm_columns = DropColumns(\n    columns=[\"NOME\"]  # Essa transforma\u00e7\u00e3o recebe como par\u00e2metro uma lista com os nomes das colunas indesejadas\n)\n\nprint(rm_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando as colunas do dataset original\nprint(\"Colunas do dataset original: \\n\")\nprint(df_data_1.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Aplicando a transforma\u00e7\u00e3o ``DropColumns`` ao conjunto de dados base\nrm_columns.fit(X=df_data_1)\n\n# Reconstruindo um DataFrame Pandas com o resultado da transforma\u00e7\u00e3o\ndf_data_2 = pd.DataFrame.from_records(\n    data=rm_columns.transform(\n        X=df_data_1\n    ),\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando as colunas do dataset transformado\nprint(\"Colunas do dataset ap\u00f3s a transforma\u00e7\u00e3o ``DropColumns``: \\n\")\nprint(df_data_2.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Nota-se que a coluna \"NOME\" foi removida e nosso dataset agora poossui apenas 17 colunas."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Transforma\u00e7\u00e3o 2: tratando dados faltantes\n\nPara tratar os dados faltantes em nosso conjunto de dados, iremos agora utilizar uma transforma\u00e7\u00e3o pronta da biblioteca scikit-learn, chamada **SimpleImputer**.\n\nEssa transforma\u00e7\u00e3o permite diversas estrat\u00e9gias para o tratamento de dados faltantes. A documenta\u00e7\u00e3o oficial pode ser encontrada em: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\nNeste exemplo iremos simplesmente transformar todos os valores faltantes em zero."}, {"metadata": {}, "cell_type": "code", "source": "# Cria\u00e7\u00e3o de um objeto ``SimpleImputer``\nsi = SimpleImputer(\n    missing_values=np.nan,  # os valores faltantes s\u00e3o do tipo ``np.nan`` (padr\u00e3o Pandas)\n    strategy='constant',  # a estrat\u00e9gia escolhida \u00e9 a altera\u00e7\u00e3o do valor faltante por uma constante\n    fill_value=0,  # a constante que ser\u00e1 usada para preenchimento dos valores faltantes \u00e9 um int64=0.\n    verbose=0,\n    copy=True\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando os dados faltantes do dataset ap\u00f3s a primeira transforma\u00e7\u00e3o (df_data_2)\nprint(\"Valores nulos antes da transforma\u00e7\u00e3o SimpleImputer: \\n\\n{}\\n\".format(df_data_2.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Aplicamos o SimpleImputer ``si`` ao conjunto de dados df_data_2 (resultado da primeira transforma\u00e7\u00e3o)\nsi.fit(X=df_data_2)\n\n# Reconstru\u00e7\u00e3o de um novo DataFrame Pandas com o conjunto imputado (df_data_3)\ndf_data_3 = pd.DataFrame.from_records(\n    data=si.transform(\n        X=df_data_2\n    ),  # o resultado SimpleImputer.transform(<<pandas dataframe>>) \u00e9 lista de listas\n    columns=df_data_2.columns  # as colunas originais devem ser conservadas nessa transforma\u00e7\u00e3o\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando os dados faltantes do dataset ap\u00f3s a segunda transforma\u00e7\u00e3o (SimpleImputer) (df_data_3)\nprint(\"Valores nulos no dataset ap\u00f3s a transforma\u00e7\u00e3o SimpleImputer: \\n\\n{}\\n\".format(df_data_3.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Nota-se que n\u00e3o temos mais nenhum valor faltante no nosso conjunto de dados :)\n\nVale salientar que nem sempre a altera\u00e7\u00e3o dos valores faltantes por 0 \u00e9 a melhor estrat\u00e9gia. O participante \u00e9 incentivado a estudar e implementar estrat\u00e9gias diferentes de tratamento dos valores faltantes para aprimorar seu modelo e melhorar sua pontua\u00e7\u00e3o final."}, {"metadata": {}, "cell_type": "markdown", "source": "### Treinando um modelo de classifica\u00e7\u00e3o"}, {"metadata": {}, "cell_type": "markdown", "source": "Finalizado o pr\u00e9-processamento, j\u00e1 temos o conjunto de dados no formato necess\u00e1rio para o treinamento do nosso modelo:"}, {"metadata": {}, "cell_type": "code", "source": "df_data_3.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "No exemplo fornecido, iremos utilizar todas as colunas, exceto a coluna **LABELS** como *features* (vari\u00e1veis de entrada).\n\nA vari\u00e1vel **LABELS** ser\u00e1 a vari\u00e1vel-alvo do modelo, conforme descrito no enunciado do desafio."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Definindo as features do modelo"}, {"metadata": {}, "cell_type": "code", "source": "# Defini\u00e7\u00e3o das colunas que ser\u00e3o features (nota-se que a coluna NOME n\u00e3o est\u00e1 presente)\nfeatures = [\n    \"MATRICULA\", 'REPROVACOES_DE', 'REPROVACOES_EM', \"REPROVACOES_MF\", \"REPROVACOES_GO\",\n    \"NOTA_DE\", \"NOTA_EM\", \"NOTA_MF\", \"NOTA_GO\",\n    \"INGLES\", \"H_AULA_PRES\", \"TAREFAS_ONLINE\", \"FALTAS\", \n]\n\n# Defini\u00e7\u00e3o da vari\u00e1vel-alvo\ntarget = [\"PERFIL\"]\n\n# Prepara\u00e7\u00e3o dos argumentos para os m\u00e9todos da biblioteca ``scikit-learn``\nX = df_data_3[features]\ny = df_data_3[target]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "O conjunto de entrada (X):"}, {"metadata": {}, "cell_type": "code", "source": "X.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As vari\u00e1veis-alvo correspondentes (y):"}, {"metadata": {}, "cell_type": "code", "source": "y.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Separando o dataset em um conjunto de treino e um conjunto de teste"}, {"metadata": {}, "cell_type": "markdown", "source": "Iremos separar o dataset fornecido em dois grupos: um para treinar nosso modelo, e outro para testarmos o resultado atrav\u00e9s de um teste cego. A separa\u00e7\u00e3o do dataset pode ser feita facilmente com o m\u00e9todo *train_test_split()* do scikit-learn:"}, {"metadata": {}, "cell_type": "code", "source": "# Separa\u00e7\u00e3o dos dados em um conjunto de treino e um conjunto de teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Criando um modelo baseado em \u00e1rvores de decis\u00e3o"}, {"metadata": {}, "cell_type": "markdown", "source": "No exemplo fornecido iremos criar um classificador baseado em **\u00e1rvores de decis\u00e3o**.\n\nO primeiro passo \u00e9 basicamente instanciar um objeto *DecisionTreeClassifier()* da biblioteca scikit-learn."}, {"metadata": {}, "cell_type": "code", "source": "# Cria\u00e7\u00e3o da \u00e1rvore de decis\u00e3o com a biblioteca ``scikit-learn``:\ndtc_model = DecisionTreeClassifier()  # O modelo ser\u00e1 criado com os par\u00e2metros padr\u00f5es da biblioteca", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Material te\u00f3rico sobre \u00e1rvores de decis\u00e3o na documenta\u00e7\u00e3o oficial do scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n\nUm guia para iniciantes no mundo do machine learning: https://developer.ibm.com/br/articles/cc-beginner-guide-machine-learning-ai-cognitive/"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Execu\u00e7\u00e3o do evento de treino de uma \u00e1rvore de decis\u00e3o"}, {"metadata": {}, "cell_type": "code", "source": "# Treino do modelo (\u00e9 chamado o m\u00e9todo *fit()* com os conjuntos de treino)\ndtc_model.fit(\n    X_train,\n    y_train\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Execu\u00e7\u00e3o de predi\u00e7\u00f5es e avalia\u00e7\u00e3o do modelo criado"}, {"metadata": {}, "cell_type": "code", "source": "# Realiza\u00e7\u00e3o de teste cego no modelo criado\ny_pred = dtc_model.predict(X_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_test.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import accuracy_score\n\n# Acur\u00e1cia alcan\u00e7ada pela \u00e1rvore de decis\u00e3o\nprint(\"Acur\u00e1cia: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "Neste notebook foi demonstrado como trabalhar com transforma\u00e7\u00f5es e modelos com a biblioteca scikit-learn. \u00c9 recomendado que o participante realize seus experimentos editando o c\u00f3digo fornecido aqui at\u00e9 que um modelo com acur\u00e1cia elevada seja alcan\u00e7ado.\n\nQuando voc\u00ea estiver satisfeito com seu modelo, pode passar para a segunda etapa do desafio -- encapsular seu modelo como uma API REST pronta para uso com o Watson Machine Learning!\n\nO notebook para a segunda etapa j\u00e1 se encontra neste projeto, basta acessar a aba **ASSETS** e inicializ\u00e1-lo! N\u00e3o se esqueca de antes desligar o Kernel deste notebook para reduzir o consumo de sua camada gr\u00e1tis do IBM Cloud Pak for Data."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}